{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook loads N-gram models (bi-gram, tri-gram, and quad-gram) combined for language identification. \n",
    "* The LID models were trained on the data from:\n",
    "\n",
    "* Vukuzenzele\n",
    "* NCHLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(lang, num, k): #returns top k n-grams according to frequency\n",
    "    lang = \" \".join(lang)\n",
    "    words = re.sub('['+string.punctuation+']', '', lang) #  punctuation removed\n",
    "    words = words.lower()\n",
    "    words = re.sub('\\s+', ' ', words).strip() # replaces multiple spaces, newline tabs with a single space\n",
    "    words = words.replace(' ','_')# so that we can visualise spaces easily\n",
    "    grams = {}\n",
    "    #print (words)\n",
    "    for i in range(len(words)-num):\n",
    "        temp = words[i:i+num]\n",
    "        if temp in grams:\n",
    "            grams[temp] += 1\n",
    "        else:\n",
    "            grams[temp] = 1\n",
    "    sum_freq = len(words) - num + 1\n",
    "    for key in grams.keys():\n",
    "        red = 1 # reduction factor equal 1 if no '_' is present\n",
    "        if '_' in key: red = 2\n",
    "        grams[key] = round(math.log(grams[key] / (red * sum_freq)), 3) #normalizing by dividing by total no of n-grams for that corpus and taking log                                             \n",
    "    grams = sorted(grams.items(), key= lambda x : x[1], reverse = True) \n",
    "    #print (grams)\n",
    "    final_grams = [] # contains a list of top k n-grams in a given language \n",
    "    log_probs = [] # contains logprobs corresponding to each n-gram\n",
    "    for i in range(len(grams)):\n",
    "        final_grams.append(grams[i][0])\n",
    "        log_probs.append(grams[i][1])\n",
    "    return final_grams, log_probs\n",
    "\n",
    "# Calculate scores\n",
    "def matching_score_2(test_grams, grams_list, n): # n helps us know whether it is bigram, trigram or quadgram\n",
    "    dist = {lang: 0 for lang in lang_list} # distance corresponding to each language\n",
    "    for gram in test_grams[0]:\n",
    "        for lang in grams_list.keys():\n",
    "            idx_2 = test_grams[0].index(gram)\n",
    "            if gram in n_grams[n][lang][0] : \n",
    "                idx = n_grams[n][lang][0].index(gram)\n",
    "                dist[lang] += abs(n_grams[n][lang][1][idx] - test_grams[1][idx_2])\n",
    "            else: # gram is not present in that language's corpus\n",
    "                dist[lang] += abs(test_grams[1][idx_2])\n",
    "                # penalty term\n",
    "    return dist \n",
    "\n",
    "\n",
    "def language_identify_2(file_address, st): # argument 'st' denotes whether you are uploading a file or directly copying text\n",
    "    test_bigrams = []\n",
    "    test_trigrams = []\n",
    "    test_quadgrams = []\n",
    "    test_file = []\n",
    "    if st == 'file': # If you are copying a file address\n",
    "        temp = file_address\n",
    "        with open(temp, 'r', errors = 'ignore') as fname: # some characters throw an error with 'utf-8'\n",
    "            file_address = fname.read()\n",
    "    #print (file_address) \n",
    "    test_bigrams = create_n_grams(file_address, 2, k)\n",
    "    test_trigrams = create_n_grams(file_address, 3, k)\n",
    "    test_quadgrams = create_n_grams(file_address, 4, k)\n",
    "    bi_dist = matching_score_2(test_bigrams, bi_grams, 2) \n",
    "    tri_dist = matching_score_2(test_trigrams, tri_grams, 3)\n",
    "    quad_dist = matching_score_2(test_quadgrams, quad_grams, 4) \n",
    "    #print (bi_dist, tri_dist)\n",
    "    final_dist = {}\n",
    "    for lang in bi_dist.keys():\n",
    "        final_dist[lang] =bi_dist[lang] + tri_dist[lang] + quad_dist[lang]\n",
    "    sum_dist = 1\n",
    "    for dist in final_dist.values():\n",
    "        sum_dist += dist\n",
    "    for lang in final_dist.keys():\n",
    "        final_dist[lang] /= sum_dist\n",
    "    dist_list = sorted(final_dist.items(), key= lambda x:x[1])     \n",
    "    #print (dist_list)    \n",
    "    # print ('Predicted language :' + dist_list[0][0] + '\\n')\n",
    "\n",
    "    for value in final_dist.values():\n",
    "             scores_unstructured.append(value)\n",
    "\n",
    "    for lang in final_dist.keys():\n",
    "            scores_structured[lang].append(final_dist[lang])\n",
    "\n",
    "    dist_list_pairs = sorted(final_dist.items())\n",
    "    # Use bounds specifuc to a language\n",
    "    # for lang in distribution_bounds.keys():\n",
    "    #             if  dist_list[0][0] == lang:\n",
    "    #                            lower_score_bound =  distribution_bounds[lang]\n",
    "    #                            if   dist_list_pairs[0][1]  < 0.075 or dist_list_pairs[0][1] > 0.125:\n",
    "    #                                          return \"Other\"\n",
    "                                                   \n",
    "    if dist_list[0][1] < 0.075 or dist_list[0][1] > 0.125:\n",
    "             return \"Other\"\n",
    "    else:\n",
    "             return dist_list[0][0]\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a model to a file using pickle.\n",
    "\n",
    "    Args:\n",
    "    model: The model to save.\n",
    "    filename (str): The filename to save the model to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Load a model from a file using pickle.\n",
    "\n",
    "    Args:\n",
    "    filename (str): The filename to load the model from.\n",
    "\n",
    "    Returns:\n",
    "    The loaded model.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def filter_sentences(text_lines, min_length=5):\n",
    "    filtered_sentences = []\n",
    "    for sentence in text_lines:\n",
    "        # Remove leading and trailing whitespace\n",
    "        text = str(sentence.strip())\n",
    "        text  = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        text = \" \".join([st for st in text.split(\" \") if st != ''])\n",
    "        # Check if the sentence is empty or too short\n",
    "        \n",
    "        if len(text.split(' ')) >= min_length:\n",
    "                    # Remove unusual characters using regular expressions  \n",
    "                    #print(text.split(' ')) \n",
    "                    filtered_sentences.append(text)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return filtered_sentences  \n",
    "\n",
    "def plot_structured(datapoints, fig_name):\n",
    "            \n",
    "            # Plotting\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            statistics = {}\n",
    "            for model, data in datapoints.items():\n",
    "                    avg = np.mean(data)\n",
    "                    std_dev = np.std(data)\n",
    "                    var = np.var(data)\n",
    "                    statistics[model] = {'mean': avg, 'std_dev': std_dev, 'variance': var}\n",
    "\n",
    "            # Extract the statistics for plotting\n",
    "            models = list(statistics.keys())\n",
    "            means = [statistics[model]['mean'] for model in models]\n",
    "            std_devs = [statistics[model]['std_dev'] for model in models]\n",
    "            variances = [statistics[model]['variance'] for model in models]   \n",
    "            print(statistics)     \n",
    "\n",
    "            # Create the plot\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot the mean as bars\n",
    "            x = np.arange(len(models))\n",
    "            bars = ax.bar(x, means, width=0.4, label='Mean', color='lightblue')\n",
    "\n",
    "            # Plot the standard deviation and variance as box and whisker plots\n",
    "            data_for_boxplot = [datapoints[model] for model in models]\n",
    "            boxplot = ax.boxplot(data_for_boxplot, positions=x, widths=0.3, patch_artist=True)\n",
    "\n",
    "            # Customize box plot colors\n",
    "            for patch, color in zip(boxplot['boxes'], ['lightgreen', 'lightgreen', 'lightgreen']):\n",
    "                patch.set_facecolor(color)\n",
    "\n",
    "            # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "            ax.set_xlabel('Languages')\n",
    "            ax.set_ylabel('Values')\n",
    "            ax.set_title('Statistics of Models')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(models)\n",
    "            ax.legend()\n",
    "\n",
    "            # Add labels for bars\n",
    "            def autolabel(rects):\n",
    "                for rect in rects:\n",
    "                    height = rect.get_height()\n",
    "                    ax.annotate(f'{height:.2f}',\n",
    "                                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                                xytext=(0, 3),  # 3 points vertical offset\n",
    "                                textcoords=\"offset points\",\n",
    "                                ha='center', va='bottom')\n",
    "\n",
    "            autolabel(bars)\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(fig_name)\n",
    "            plt.show()\n",
    "\n",
    "def plot_unstructured(datapoints, fig_name):\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Scatter plot\n",
    "        plt.scatter(range(len(datapoints)), datapoints, c='gray', alpha=0.5, s=5) \n",
    "        # KDE plot\n",
    "        sns.kdeplot(x=range(len(datapoints)), y=datapoints, cmap='viridis', shade=True, bw_adjust=0.5)\n",
    "        # Labels and title\n",
    "        plt.xlabel('N-gram Model')\n",
    "        plt.ylabel('Scores')\n",
    "        plt.title('Unstructured scores from the models')\n",
    "    \n",
    "        # Legend\n",
    "        plt.legend()\n",
    "        # Grid\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.savefig(fig_name)\n",
    "\n",
    "def Remove_Layer(sentence):\n",
    "      text = ''\n",
    "      # return text and urls\n",
    "\n",
    "      # check single or double character\n",
    "      pattern = r'^\\s*(\\S{1,2}(\\s+\\S{1,2})*)?\\s*$'\n",
    "      match = re.fullmatch(pattern, sentence)\n",
    "      if match:\n",
    "            return ''\n",
    "      \n",
    "      else: \n",
    "            return sentence \n",
    "            \n",
    "\n",
    "                             \n",
    "\n",
    "def evaluate(path_to_data):\n",
    "        for item in os.listdir(path_to_data):\n",
    "            # Construct the full path to the current item\n",
    "            item_path = os.path.join(path_to_data, item)\n",
    "            # Check if the current item is a directory\n",
    "            if os.path.isdir(item_path):\n",
    "                # Loop through the contents of the subfolder\n",
    "                for sub_item in os.listdir(item_path):\n",
    "                    # Construct the full path to the sub-item\n",
    "                    sub_item_path = os.path.join(item_path, sub_item)\n",
    "\n",
    "                    # Check if the sub-item is a file or directory\n",
    "                    if os.path.isdir(sub_item_path):\n",
    "                        lang = item_path.split(\"/\")[-1]\n",
    "                        files_manager = {}\n",
    "                        for file_name in os.listdir(sub_item_path):\n",
    "                                file_path = os.path.join(sub_item_path, file_name)\n",
    "\n",
    "                                # Check if the current item is a file\n",
    "                                if os.path.isfile(file_path):\n",
    "                                            #print(\"File:\", file_path)\n",
    "                                            if 'train' in file_name:\n",
    "                                                    files_manager['train_file'] = file_path\n",
    "                                            elif 'dev' in file_name:\n",
    "                                                    files_manager['dev_file'] = file_path\n",
    "                                            elif 'test' in file_name:\n",
    "                                                    files_manager['test_file'] = file_path  \n",
    "                                            else:\n",
    "                                                    print(\"This file is unknow\")                              \n",
    "\n",
    "                        # Dev\n",
    "                        if  files_manager['test_file']:    \n",
    "                                    corpus   = open(files_manager['test_file'], \"r\")\n",
    "                                    corpus = corpus.readlines()\n",
    "                                    for line in corpus:\n",
    "                                                 l_identified = language_identify_2(line, 'string')\n",
    "                                                 if l_identified == lang:\n",
    "                                                            evals[lang][0] +=1\n",
    "                                    evals[lang][1] = len(corpus)\n",
    "\n",
    "                        # Dev\n",
    "                        if  files_manager['dev_file']:    \n",
    "                                    corpus   = open(files_manager['dev_file'], \"r\")\n",
    "                                    corpus = corpus.readlines()\n",
    "                                    for line in corpus:\n",
    "                                                 l_identified = language_identify_2(line, 'string')\n",
    "                                                 if l_identified == lang:\n",
    "                                                            dev_evals[lang][0] +=1\n",
    "                                    dev_evals[lang][1] = len(corpus)            \n",
    "        return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_root_path = \"\" # path to test data \n",
    "train_data_root_path = \"\" # when creating new models\n",
    "models_root_path = '../../../../../../../ext_data/thapelo/All-Grams-Ablation-Models/9/'  # map to folder containing the models in repo\n",
    "bi_sorted        = \"\" # path to where output should be stored\n",
    "n = [2,3,4]  # Here we are choosing bigrams,trigrams and quadgrams; change this value to get n-grams with a particular n\n",
    "k = 50 # Decides how many top n-grams will be used for calculating the distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        bi_grams = {}\n",
    "        tri_grams = {}\n",
    "        quad_grams = {}\n",
    "        lang_list = []\n",
    "        lang_list = ['ssw', 'eng', 'xho', 'zul', 'tsn', 'af', 'sot', 'tso', 'nso', 'nbl', 'ven']\n",
    "\n",
    "        n_grams = load_model('../../../../../../../ext_data/thapelo/All-Grams-Ablation-Models/9/all_gram_model.pkl')\n",
    "        bi_grams = load_model('../../../../../../../ext_data/thapelo/All-Grams-Ablation-Models/9/bigram_model.pkl')\n",
    "        tri_grams = load_model('../../../../../../../ext_data/thapelo/All-Grams-Ablation-Models/9/trigram_model.pkl')\n",
    "        quad_grams = load_model('../../../../../../../ext_data/thapelo/All-Grams-Ablation-Models/9/trigram_model.pkl')\n",
    "\n",
    "\n",
    "        evals = {lang:[0, 0] for lang in lang_list}\n",
    "        dev_evals = {lang:[0, 0] for lang in lang_list}\n",
    "        # evaluation_scores = evaluate(eval_data_root_path)     # calculate evaluation scores\n",
    "\n",
    "\n",
    "        # # capture scores\n",
    "        scores_unstructured  = []\n",
    "        scores_structured    = {lang: []  for lang in ['ssw', 'eng', 'xho', 'zul', 'tsn', 'af', 'sot', 'tso', 'nso', 'nbl', 'ven']}\n",
    "\n",
    "        # Loop through folfer co\n",
    "\n",
    "        # Test sentences\n",
    "        c_root_folder = ''   # add path to data\n",
    "        for root, dirs, files in os.walk(c_root_folder):\n",
    "                for file in files:\n",
    "                        # Construct the full path to the file\n",
    "                        text_path = os.path.join(root, file)\n",
    "                        \n",
    "                        # Open the file\n",
    "                        with open(text_path, 'r') as f:\n",
    "                            # Read the contents of the file\n",
    "                            sentences = f.readlines()\n",
    "\n",
    "                        # filter sentences\n",
    "                        filtered_sentences = filter_sentences(sentences)\n",
    "\n",
    "                        # Identify language for each sentence\n",
    "                        for sentence in filtered_sentences:\n",
    "                            # identified_language = identify_language(sentence, language_models)\n",
    "                            if len(\"\".join(filter(lambda x: not x.isdigit(), sentence)).lower().split()) > 5:\n",
    "                                        sentence = Remove_Layer(sentence)\n",
    "                                        if sentence != '':\n",
    "                                                identified_language = language_identify_2(\"\".join(filter(lambda x: not x.isdigit(), sentence)).lower(), 'string')\n",
    "                                                if identified_language == 'nso':\n",
    "                                                        with open(bi_sorted + 'n_gram_filtered_nso.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')\n",
    "                                                elif  identified_language == 'tsn':\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_tsn.txt', 'a') as tsn_file:\n",
    "                                                                        tsn_file.write(sentence + '\\n') \n",
    "                                                elif identified_language == \"sot\":\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_sot.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')\n",
    "                                                elif identified_language == 'xho':\n",
    "                                                        with open(bi_sorted + 'n_gram_filtered_xho.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')   \n",
    "                                                elif identified_language == 'zul':\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_zul.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')   \n",
    "                                                elif identified_language == \"ssw\":\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_ssw.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')   \n",
    "                                                elif identified_language == \"ven\":\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_ven.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n') \n",
    "                                                elif identified_language == 'tso':\n",
    "                                                        with open(bi_sorted + 'n_gram_filtered_tso.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')   \n",
    "                                                elif identified_language == 'af':\n",
    "                                                        with open(bi_sorted + 'n_gram_filtered_af.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')                                                                                                                                   \n",
    "                                                elif identified_language == 'nbl':\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_nbl.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')\n",
    "                                                elif identified_language == 'eng':\n",
    "                                                    with open(bi_sorted + 'n_gram_filtered_en.txt', 'a') as n_file:\n",
    "                                                                        n_file.write(sentence + '\\n')  \n",
    "                                                                            \n",
    "                                                else:\n",
    "                                                    continue \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
